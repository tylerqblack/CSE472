{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import tree\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "def train(classifier, name, param_grid=None) :\n",
    "    start_time = time.time()\n",
    "    if param_grid == None :\n",
    "        classifier.fit(X_train, y_train)\n",
    "        results[name] = dict(model=classifier)\n",
    "    else :\n",
    "        grid = GridSearchCV(classifier, param_grid, cv=10, scoring='accuracy', n_jobs=2) # Do a 10-fold cross validation\n",
    "        grid.fit(X, y) # fit the grid with data\n",
    "        results[name] = dict(grid=grid, model=classifier)\n",
    "    #total_time = datetime.datetime.fromtimestamp(time.time() - start_time)\n",
    "    total_time = datetime.timedelta(seconds=time.time() - start_time)\n",
    "    print(\"Training time : \" + str(total_time))#.strftime('%H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3037\n",
      "3037\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Load N features and add Label, and make label y\n",
    "bot_features = pd.read_pickle(\"bot_features.pkl\")\n",
    "bot_number = len(bot_features)\n",
    "\n",
    "hum_features = pd.read_pickle(\"hum_features.pkl\")\n",
    "hum_number = len(hum_features)\n",
    "\n",
    "features = pd.concat([bot_features, hum_features])\n",
    "feature_number = len(features)\n",
    "X = features.values.tolist()\n",
    "y = []\n",
    "for i in range(0, bot_number) :\n",
    "    y.append(False)\n",
    "for i in range(bot_number, feature_number) :\n",
    "    y.append(True)\n",
    "\n",
    "print(len(X))\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divise dataset\n",
    "def divide_dataset(X, y) :\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "    return (X_train, X_test, y_train, y_test)\n",
    "\n",
    "X_train, X_test, y_train, y_test = divide_dataset(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time : 0:00:02.053016\n"
     ]
    }
   ],
   "source": [
    "results = dict()\n",
    "\n",
    "name = \"k-NN\"\n",
    "classifier = KNeighborsClassifier(weights='uniform')\n",
    "k_range = list(range(1, 31)) # list of parameter values to test\n",
    "param_grid = dict(n_neighbors=k_range)\n",
    "train(classifier, name, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time : 0:00:00.062830\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "name = \"Neural net\"\n",
    "#classifier = MLPClassifier(alpha=1)\n",
    "classifier = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)\n",
    "train(classifier, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+----------+-----------+--------+-------+-------+-------+\n",
      "|   Model    | Best score | accuracy | precision | recall |  F-M. |  MCC  |  AUC  |\n",
      "+------------+------------+----------+-----------+--------+-------+-------+-------+\n",
      "| Neural net |   0.821    |  0.821   |   0.823   | 0.912  | 0.865 | 0.606 | 0.788 |\n",
      "|    k-NN    |   0.808    |  0.817   |   0.835   | 0.885  | 0.859 | 0.599 | 0.792 |\n",
      "+------------+------------+----------+-----------+--------+-------+-------+-------+\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "import operator\n",
    "from sklearn import metrics\n",
    "import math\n",
    "t = PrettyTable(['Model', 'Best score', 'accuracy', 'precision', 'recall', 'F-M.', 'MCC', 'AUC'])\n",
    "for clf_name, result in results.items() :\n",
    "    model = result['model']\n",
    "    if 'grid' in result :\n",
    "        grid = result['grid']\n",
    "        score = grid.best_score_\n",
    "        # Compute false positives and false negatives\n",
    "        model.__init__(**grid.best_params_)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        #print(result.best_estimator_)\n",
    "    else : # For non grid_search models\n",
    "        #training_error = clf.score(X_train, y_train)\n",
    "        score = model.score(X_test, y_test)\n",
    "        y_pred = model.predict(X_test)\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    #print(clf_name + \" tn=\" + str(tn) + \" fp=\" + str(fp) + \" fn=\" + str(fn) + \" tp=\" + str(tp))\n",
    "    accuracy = float(tp + tn) / (tp + tn + fp + fn)\n",
    "    precision = float(tp) / (tp + fp)\n",
    "    recall = float(tp) / (tp + fn) # a.k.a. sensitivity\n",
    "    f_measure = float(2 * precision * recall) / (precision + recall)\n",
    "    mcc = -1\n",
    "    if fp!=0 and tp != 0 and tn != 0 and fn!= 0:\n",
    "        mcc = float(tp * tn - fp * fn) / math.sqrt(float(tp+fn) * (tp+fp) * (tn+fp) * (tn+fn)) # Matthew Correlation Coefficient\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    t.add_row([clf_name, round(score, 3), round(accuracy, 3), round(precision,3), round(recall,3), round(f_measure,3), round(mcc,3), round(auc,3)]) #fp, tn, fn, tp])\n",
    "\n",
    "        \n",
    "print(t.get_string(sort_key=operator.itemgetter(2, 1), sortby=\"Best score\", reversesort=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
