{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import tree\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "def train(classifier, name, param_grid=None) :\n",
    "    start_time = time.time()\n",
    "    if param_grid == None :\n",
    "        classifier.fit(X_train, y_train)\n",
    "        results[name] = dict(model=classifier)\n",
    "    else :\n",
    "        grid = GridSearchCV(classifier, param_grid, cv=10, scoring='accuracy', n_jobs=2) # Do a 10-fold cross validation\n",
    "        grid.fit(X, y) # fit the grid with data\n",
    "        results[name] = dict(grid=grid, model=classifier)\n",
    "    #total_time = datetime.datetime.fromtimestamp(time.time() - start_time)\n",
    "    total_time = datetime.timedelta(seconds=time.time() - start_time)\n",
    "    print(\"Training time : \" + str(total_time))#.strftime('%H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       {'friends/follower_ratio': 39.888888888888886,...\n",
      "1       {'friends/follower_ratio': 0.16568559556786702...\n",
      "2       {'friends/follower_ratio': 0.1044140625, 'bot_...\n",
      "3       {'friends/follower_ratio': 0.5298765432098765,...\n",
      "4       {'friends/follower_ratio': 183.5, 'bot_in_biog...\n",
      "                              ...                        \n",
      "1945    {'friends/follower_ratio': 0.00286612426035502...\n",
      "1946    {'friends/follower_ratio': 0.11363636363636363...\n",
      "1947    {'friends/follower_ratio': 0, 'bot_in_biograph...\n",
      "1948    {'friends/follower_ratio': 1.0, 'bot_in_biogra...\n",
      "1949    {'friends/follower_ratio': 0.00276783467666658...\n",
      "Name: features, Length: 3038, dtype: object\n",
      "3038\n",
      "3038\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Load N features and add Label, and make label y\n",
    "bot = pd.read_pickle(\"bot_features.pkl\")\n",
    "bot_features = bot['features']\n",
    "bot_number = len(bot_features)\n",
    "#print(bot_features)\n",
    "\n",
    "hum = pd.read_pickle(\"hum_features.pkl\")\n",
    "hum_features = hum['features']\n",
    "hum_number = len(hum_features)\n",
    "#print(hum_features)\n",
    "\n",
    "features = pd.concat([bot_features, hum_features])\n",
    "feature_number = len(features)\n",
    "print(features)\n",
    "\n",
    "X = features.values\n",
    "X = [list(feature.values()) for feature in X]\n",
    "\n",
    "y = []\n",
    "for i in range(0, bot_number) :\n",
    "    y.append(False)\n",
    "for i in range(bot_number, feature_number) :\n",
    "    y.append(True)\n",
    "\n",
    "print(len(X))\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divise dataset\n",
    "def divide_dataset(X, y) :\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "    return (X_train, X_test, y_train, y_test)\n",
    "\n",
    "X_train, X_test, y_train, y_test = divide_dataset(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time : 0:00:03.488848\n"
     ]
    }
   ],
   "source": [
    "results = dict()\n",
    "\n",
    "name = \"k-NN\"\n",
    "classifier = KNeighborsClassifier(weights='uniform')\n",
    "k_range = list(range(1, 31)) # list of parameter values to test\n",
    "param_grid = dict(n_neighbors=k_range)\n",
    "train(classifier, name, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time : 0:00:00.287048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paris\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "name = \"Neural net\"\n",
    "#classifier = MLPClassifier(alpha=1)\n",
    "classifier = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)\n",
    "train(classifier, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time : 0:00:24.162894\n"
     ]
    }
   ],
   "source": [
    "name = \"Random forest\"\n",
    "classifier = RandomForestClassifier()\n",
    "d_range = list(range(1, 31)) # list of parameter values to test\n",
    "#s_range = list(range(2, 10))\n",
    "param_grid = dict(max_depth=d_range)#, min_samples_split=s_range)\n",
    "train(classifier, name, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time : 0:00:00.031913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paris\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "name = \"Log. Regression\"\n",
    "classifier = LogisticRegression()\n",
    "train(classifier, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------+----------+-----------+--------+-------+-------+-------+\n",
      "|      Model      | Best score | accuracy | precision | recall |  F-M. |  MCC  |  AUC  |\n",
      "+-----------------+------------+----------+-----------+--------+-------+-------+-------+\n",
      "|  Random forest  |    0.96    |  0.958   |    0.96   | 0.975  | 0.967 |  0.91 | 0.952 |\n",
      "| Log. Regression |   0.918    |  0.918   |   0.908   | 0.968  | 0.937 | 0.824 |  0.9  |\n",
      "|    Neural net   |   0.909    |  0.909   |   0.893   | 0.973  | 0.931 | 0.805 | 0.887 |\n",
      "|       k-NN      |   0.883    |  0.894   |   0.894   | 0.945  | 0.919 | 0.771 | 0.876 |\n",
      "+-----------------+------------+----------+-----------+--------+-------+-------+-------+\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "import operator\n",
    "from sklearn import metrics\n",
    "import math\n",
    "t = PrettyTable(['Model', 'Best score', 'accuracy', 'precision', 'recall', 'F-M.', 'MCC', 'AUC'])\n",
    "for clf_name, result in results.items() :\n",
    "    model = result['model']\n",
    "    if 'grid' in result :\n",
    "        grid = result['grid']\n",
    "        score = grid.best_score_\n",
    "        # Compute false positives and false negatives\n",
    "        model.__init__(**grid.best_params_)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        #print(result.best_estimator_)\n",
    "    else : # For non grid_search models\n",
    "        #training_error = clf.score(X_train, y_train)\n",
    "        score = model.score(X_test, y_test)\n",
    "        y_pred = model.predict(X_test)\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    #print(clf_name + \" tn=\" + str(tn) + \" fp=\" + str(fp) + \" fn=\" + str(fn) + \" tp=\" + str(tp))\n",
    "    accuracy = float(tp + tn) / (tp + tn + fp + fn)\n",
    "    precision = float(tp) / (tp + fp)\n",
    "    recall = float(tp) / (tp + fn) # a.k.a. sensitivity\n",
    "    f_measure = float(2 * precision * recall) / (precision + recall)\n",
    "    mcc = -1\n",
    "    if fp!=0 and tp != 0 and tn != 0 and fn!= 0:\n",
    "        mcc = float(tp * tn - fp * fn) / math.sqrt(float(tp+fn) * (tp+fp) * (tn+fp) * (tn+fn)) # Matthew Correlation Coefficient\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    t.add_row([clf_name, round(score, 3), round(accuracy, 3), round(precision,3), round(recall,3), round(f_measure,3), round(mcc,3), round(auc,3)]) #fp, tn, fn, tp])\n",
    "\n",
    "        \n",
    "print(t.get_string(sort_key=operator.itemgetter(2, 1), sortby=\"Best score\", reversesort=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset used for unlabeled sample\n",
      "829\n",
      "Info of unlabeled_data\n",
      "             id      screen_name  \\\n",
      "0    2718436417        DJGruuvan   \n",
      "1    3287012484     penelope20mn   \n",
      "2      93816184       flipcritic   \n",
      "3    3027809025       leitmarvin   \n",
      "4    4462881555  ShitpostBot5000   \n",
      "..          ...              ...   \n",
      "344  4518999021  shakethatbrass2   \n",
      "345  3231781692       tweegeemee   \n",
      "346  3317501195    rightnowbbcr2   \n",
      "347   989135184   rightnow6music   \n",
      "348  2433869952     trashcanlife   \n",
      "\n",
      "                                              features  \n",
      "0    {'friends/follower_ratio': 0.01036855500047129...  \n",
      "1    {'friends/follower_ratio': 2.735031447908205e-...  \n",
      "2    {'friends/follower_ratio': 0.00011402278394249...  \n",
      "3    {'friends/follower_ratio': 0.00455166135639508...  \n",
      "4    {'friends/follower_ratio': 4.228058749289927e-...  \n",
      "..                                                 ...  \n",
      "344  {'friends/follower_ratio': 0.2, 'bot_in_biogra...  \n",
      "345  {'friends/follower_ratio': 0.00062679999415230...  \n",
      "346  {'friends/follower_ratio': 0.02083333333333333...  \n",
      "347  {'friends/follower_ratio': 1.2974038948064922e...  \n",
      "348  {'friends/follower_ratio': 0.0, 'bot_in_biogra...  \n",
      "\n",
      "[349 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "with open(\"./datasets/Un_crawled_data/unlabeled.tsv\", \"r\") as f :\n",
    "    TSV = f.readlines()\n",
    "classes = {\"human\":1, \"bot\":0}\n",
    "dataset = []\n",
    "for label in TSV:\n",
    "    #All user_ids in TSV are bot.\n",
    "    user_id = int(label.split()[0])\n",
    "    label = label.split()[1]\n",
    "    dataset.append([user_id, classes[label]])\n",
    "print(\"Length of dataset used for unlabeled sample\")\n",
    "print(len(dataset))\n",
    "\n",
    "print(\"Info of unlabeled_data\")\n",
    "unlabeled = pd.read_pickle(\"unlabeled_features.pkl\")\n",
    "unlabeled_number = len(unlabeled)\n",
    "print(unlabeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def maskData(true_labels, percentage):\n",
    "\n",
    "    mask = np.ones((1,len(true_labels)),dtype=bool)[0]\n",
    "    labels = true_labels.copy()\n",
    "    \n",
    "    for l, enc in zip(np.unique(true_labels),range(0,len(np.unique(true_labels)))):\n",
    "        \n",
    "        deck = np.argwhere(true_labels == l).flatten()        \n",
    "        random.shuffle(deck)\n",
    "        \n",
    "        mask[deck[:int(percentage * len(true_labels[true_labels == l]))]] = False\n",
    "\n",
    "        labels[labels == l] = enc\n",
    "\n",
    "    labels[mask] = -1\n",
    "    \n",
    "    return np.array(labels).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semi supervised Learning by Label Propagation (Ratio of labeled:unlabeled is about 7:3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.semi_supervised import LabelPropagation\n",
    "\n",
    "features = unlabeled[\"features\"]\n",
    "\n",
    "X_unlabeled = features.values\n",
    "X_unlabeled = [list(feature.values()) for feature in X_unlabeled]\n",
    "\n",
    "y_unlabeled = list()\n",
    "\n",
    "for i in range(unlabeled_number):\n",
    "    y_unlabeled.append(True)\n",
    "y_unlabeled = np.array(y_unlabeled)\n",
    "\n",
    "y_unlabeled = maskData(y_unlabeled, 0.05)\n",
    "y_unlabeled = y_unlabeled.tolist()\n",
    "y = y_test+y_unlabeled\n",
    "X = X_test+X_unlabeled\n",
    "\n",
    "print(\"Semi supervised Learning by Label Propagation (Ratio of labeled:unlabeled is about 7:3)\")\n",
    "model = LabelPropagation()\n",
    "model.fit(X, y)\n",
    "pred = np.array(model.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human/Bot ratio in Test data\n",
      "965 387\n",
      "semi-supervised learning result\n",
      "0.9948224852071006\n"
     ]
    }
   ],
   "source": [
    "count= 0\n",
    "hum=0\n",
    "bot=0\n",
    "labels = y\n",
    "for i, j in zip(labels, pred) :\n",
    "    if (i==j) :\n",
    "        count+=1\n",
    "    if i==1:\n",
    "        hum+=1\n",
    "    if i==0:\n",
    "        bot+=1\n",
    "print(\"Human/Bot ratio in Test data\")\n",
    "print(hum, bot)\n",
    "print(\"semi-supervised learning result\")\n",
    "print(count/len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
